{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPKDxunM2OO4RgqSSjjar28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1EOYbBpo1lQu"},"source":["# Word-based RNN\n","We define a word-based RNN for text generation. We use LSTM cells and word2vec for embedding.\n","\n","References:\n","\n","[1] [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n","\n","[2] [Word2Vec + LSTM](https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ivq2CKjA4BWB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629057751876,"user_tz":-120,"elapsed":9,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"589c5733-7a76-4d1e-da5e-e8284fd7628f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pTHiCmPh4Hb7"},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.models import Sequential\n","import re\n","import string\n","from tqdm import tqdm\n","import pickle\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","PATH = '/content/drive/MyDrive/Datasets/'\n","rnn_dir = '/content/drive/MyDrive/NLP/RNNs/'\n","embedding_dim = 256\n","vocab_size = 2**14"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxOH0rhu4JFV"},"source":["# Load word2vec model to use as an embedding layer.\n","# See word2vec.ipynb for details.\n","word2vec = keras.models.load_model(rnn_dir + 'w2v_model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_lp8BEbVEap","executionInfo":{"status":"ok","timestamp":1629057767857,"user_tz":-120,"elapsed":8,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"b9946df0-c311-4e7e-cf07-c272b88d417d"},"source":["word2vec.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"word2_vec\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","w2v_embedding (Embedding)    multiple                  4194304   \n","_________________________________________________________________\n","embedding (Embedding)        multiple                  4194304   \n","=================================================================\n","Total params: 8,388,608\n","Trainable params: 8,388,608\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xI_rqAxPZas7"},"source":["# Load saved vocabulary.\n","with open(rnn_dir + 'vocab_cfg.plk', 'rb') as f:\n","    vec_cfg = pickle.load(f)\n","with open(rnn_dir + 'vocab_voc.plk', 'rb') as f:\n","    vec_voc = pickle.load(f)\n","with open(rnn_dir + 'vocab_wgt.plk', 'rb') as f:\n","    vec_wgt = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PFDi0D3dHcH","executionInfo":{"status":"ok","timestamp":1629057771529,"user_tz":-120,"elapsed":71,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"4b36a5be-b6cc-4ee9-de22-32330e2a77bf"},"source":["vectorize_layer = layers.experimental.preprocessing.TextVectorization.from_config(vec_cfg)\n","vectorize_layer.set_vocabulary(vec_voc)\n","vectorize_layer.set_weights(vec_wgt)\n","vectorize_layer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f8f26577ad0>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Tdla5Sa93UhA"},"source":["Because of a mistake in a vocabulary generation, we don't have an embedding for \"\\n\". We use a dirty (but effective) hack to solve this problem.\n","\n","We map any occurance of \"\\n\" in input data to the least frequent word (LSW) in our vocabulary. Since LSW occurs only single-digit number of times in our corpus, we won't get any bad behaviour.\n","\n","Before outputing prediction, we replace all occurances of LSW to \"\\n\" character."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"CKJwFusneLFM","executionInfo":{"status":"ok","timestamp":1629057771551,"user_tz":-120,"elapsed":89,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"7efed2d4-1351-4a79-b79f-dcc501571fe8"},"source":["lfw = vec_voc[-1]\n","lfw"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'geweest'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"khbsXqGGljRO","executionInfo":{"status":"ok","timestamp":1629057795242,"user_tz":-120,"elapsed":4176,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"9991ddd5-4578-441f-e2c5-004c8a4929d8"},"source":["# Read the data.\n","df = pd.read_csv(PATH + 'kaggle_rock_new.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>lyrics</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>a lot of cats are hatin' slandering makin' bad...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>somebody tell me why we landed here on the pla...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>i'm spittin' with the venom\\nto your soul thro...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>where should i begin cripplin' all you villain...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>enough of all that let's switch up the format\\...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>100252</th>\n","      <td>100252</td>\n","      <td>break down we've got to make them see\\nno disc...</td>\n","    </tr>\n","    <tr>\n","      <th>100253</th>\n","      <td>100253</td>\n","      <td>everything comes to a question where time is t...</td>\n","    </tr>\n","    <tr>\n","      <th>100254</th>\n","      <td>100254</td>\n","      <td>you got to climb up on your high horses decide...</td>\n","    </tr>\n","    <tr>\n","      <th>100255</th>\n","      <td>100255</td>\n","      <td>it all comes tumbling down\\nno vital parts rem...</td>\n","    </tr>\n","    <tr>\n","      <th>100256</th>\n","      <td>100256</td>\n","      <td>when a life breaks it is silent\\nit just withe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100257 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["        Unnamed: 0                                             lyrics\n","0                0  a lot of cats are hatin' slandering makin' bad...\n","1                1  somebody tell me why we landed here on the pla...\n","2                2  i'm spittin' with the venom\\nto your soul thro...\n","3                3  where should i begin cripplin' all you villain...\n","4                4  enough of all that let's switch up the format\\...\n","...            ...                                                ...\n","100252      100252  break down we've got to make them see\\nno disc...\n","100253      100253  everything comes to a question where time is t...\n","100254      100254  you got to climb up on your high horses decide...\n","100255      100255  it all comes tumbling down\\nno vital parts rem...\n","100256      100256  when a life breaks it is silent\\nit just withe...\n","\n","[100257 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EjBxgp2FFwva","executionInfo":{"status":"ok","timestamp":1629057795252,"user_tz":-120,"elapsed":28,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"c302004f-6a78-421e-c8b3-f439ffc7efd7"},"source":["def standardize(lyrics):\n","    lyrics = lyrics.lower()\n","    lyrics = lyrics.replace('\\n', ' ' + lfw + ' ')\n","    illegal = string.punctuation.replace(\"'\", '')  # ' is legal\n","    lyrics = lyrics.translate(str.maketrans('', '', illegal))\n","    return lyrics\n","\n","standardize(\"Hey!\\nI'm a transformer!\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"hey geweest i'm a transformer\""]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZojMQJgGIoK","executionInfo":{"status":"ok","timestamp":1629057795802,"user_tz":-120,"elapsed":7,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"7c0c6676-3304-4220-850c-f9087b9eee5f"},"source":["sequence_length = 16\n","\n","def split_list(l, n):\n","    # Split list l in n equal parts.\n","    return [l[i:i+n] for i in range(0, len(l), n)]\n","\n","print(split_list([1, 2, 3, 4, 5], 2))\n","\n","# We want RNN to predict a new word based on\n","# what it had seen so far. We build a training\n","# sample the following way:\n","# Given a line, e.g. \"here comes the sun\",\n","# we set everything up until the last word as\n","# a training sample (\"here comes the\"),\n","# and everything except the first word as a\n","# label (\"comes the sun\").\n","\n","def get_training_data():\n","    iter = df['lyrics'].iteritems()\n","    for raw_lyric in tqdm(iter):\n","        lyric = standardize(raw_lyric[1])\n","        split = split_list(lyric.split(' '), sequence_length)\n","        \n","        for lyric in split:\n","            x = lyric[:-1]\n","            # y = lyric[1:]\n","            y = lyric[-1]\n","\n","\n","            train_x = vectorize_layer.call([' '.join(x)])[0]\n","            train_y = vectorize_layer.call([' '.join(y)])[0]\n","\n","            yield np.array(train_x), np.array(train_y)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1, 2], [3, 4], [5]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mizEmzqgJP7J","executionInfo":{"status":"ok","timestamp":1629057907881,"user_tz":-120,"elapsed":109687,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"1de39afa-9d8a-4642-c26c-5995d0bdf5be"},"source":["BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","dataset = tf.data.Dataset.from_generator(get_training_data,\n","                                         output_shapes=((16,), (16,)),\n","                                         output_types=(tf.int64, tf.int64))\n","\n","# Dataset performance optimization:\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","print(dataset)\n","list(dataset.take(1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<PrefetchDataset shapes: ((1024, 16), (1024, 16)), types: (tf.int64, tf.int64)>\n"],"name":"stdout"},{"output_type":"stream","text":["791it [01:49,  7.25it/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[(<tf.Tensor: shape=(1024, 16), dtype=int64, numpy=\n","  array([[    2,   245,    42, ...,    15,   373,     0],\n","         [   31,     4,  2017, ...,     0,     0,     0],\n","         [ 1030,     6,  1810, ..., 16383,   220,     0],\n","         ...,\n","         [   68,     7,   137, ...,   343,     4,     0],\n","         [  118,    66,  1168, ...,  3615,    57,     0],\n","         [  481,    19,    10, ...,   105,   166,     0]])>,\n","  <tf.Tensor: shape=(1024, 16), dtype=int64, numpy=\n","  array([[1273, 1815, 1670, ...,    0,    0,    0],\n","         [1979,  547,  547, ...,    0,    0,    0],\n","         [1016,  547, 1670, ...,    0,    0,    0],\n","         ...,\n","         [   7, 1670,    0, ...,    0,    0,    0],\n","         [1670,  388,    0, ...,    0,    0,    0],\n","         [ 641, 3479,  641, ...,    0,    0,    0]])>)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"deqO_dq0VGI6"},"source":["model = Sequential()\n","model.add(word2vec.get_layer('w2v_embedding'))\n","model.add(keras.layers.LSTM(units=embedding_dim))\n","model.add(keras.layers.Dense(units=vocab_size))\n","model.add(keras.layers.Activation('softmax'))\n","loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n","model.compile(optimizer='adam', loss=loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbfGPGsLNYFb","executionInfo":{"status":"ok","timestamp":1629058167075,"user_tz":-120,"elapsed":6,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"d58db38f-bc81-4fce-e575-33f787a7ab46"},"source":["def idx2word(i):\n","    return vec_voc[i]\n","\n","def word2idx(w):\n","    return int(vectorize_layer.call([w])[0][0])\n","\n","print(idx2word(100))\n","print(word2idx('only'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["only\n","100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nMy6CyRilCHn"},"source":["In the following code we use [temperature based random sampling](https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f://)"]},{"cell_type":"code","metadata":{"id":"5JjSkpPZfCz3"},"source":["# Adapted from [2]\n","\n","def sample(preds, temperature=1.0):\n","  # Temperature based random sampling.\n","  if temperature <= 0:\n","    return np.argmax(preds)\n","  preds = np.asarray(preds).astype('float64')\n","  preds = np.log(preds) / temperature\n","  exp_preds = np.exp(preds)\n","  preds = exp_preds / np.sum(exp_preds)\n","  probas = np.random.multinomial(1, preds, 1)\n","  return np.argmax(probas)\n","\n","def generate_next(text, num_generated=10):\n","  # Generate lyrics based on prompt.\n","\n","  # Vectorize.\n","  word_idxs = [word2idx(word) for word in text.lower().split()]\n","  for i in range(num_generated):\n","    prediction = model.predict(x=np.array(word_idxs))\n","\n","    # Temperature based random sampling.\n","    idx = sample(prediction[-1], temperature=0.7)\n","    word_idxs.append(idx)\n","\n","  # Devectorize.\n","  result = ' '.join(idx2word(idx) for idx in word_idxs)\n","  return result.replace(lfw, '\\n')\n","\n","def on_epoch_end(epoch, _):\n","  # Generate text with the following prompts to\n","  # see progress on each epoch.\n","  print('\\nGenerating text after epoch: %d' % epoch)\n","  texts = [\n","    'here comes the sun little darling\\n',\n","    'empty spaces what are we living for\\n',\n","    'ticking away the moments that make up a dull day\\n'\n","  ]\n","  for text in texts:\n","    sample = generate_next(text)\n","    print('%s... -> %s' % (text, sample))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZFIFxxqoGUL"},"source":["Unfortunately, we couldn't figure out what's wrong and why the model refuses to train."]},{"cell_type":"code","metadata":{"id":"JJe3aap7fE4N","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1629058187955,"user_tz":-120,"elapsed":12335,"user":{"displayName":"NLP TUM","photoUrl":"","userId":"00384399602368934316"}},"outputId":"b8070b4e-0987-4658-b436-37752d523ba6"},"source":["# Train the model.\n","model.fit(dataset,\n","          batch_size=128,\n","          epochs=20,\n","          callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-9f57a2827377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:771 train_step  *\n        loss = self.compiled_loss(\n    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:142 __call__  *\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:246 call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:155 __call__  **\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:259 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:1713 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4980 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:4229 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:4136 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (16384,)) should equal the shape of logits except for the last dimension (received (1024, 16384)).\n"]}]}]}